{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtSOZOAHxFx6pYoQjm/p43"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vhNIUiRACIjR","executionInfo":{"status":"ok","timestamp":1682124062530,"user_tz":-420,"elapsed":25714,"user":{"displayName":"Andi Syamsul","userId":"15911607600703468816"}},"outputId":"2b9bbc5f-6767-423e-b14e-b850d439ec58"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# imoprting data\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# load data\n","df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/College/Research Haikal/dataset1.xlsx', index_col=0)"],"metadata":{"id":"fAYfzCi-Czxb","executionInfo":{"status":"ok","timestamp":1682124070429,"user_tz":-420,"elapsed":2633,"user":{"displayName":"Andi Syamsul","userId":"15911607600703468816"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# create a dictionary to translate column names\n","translation_dict = {'Wavelenth': 'Wavelength', \n","                    'indeks bias': 'Refractive index',\n","                    'diameter hole': 'Hole diameter', \n","                    'ketebalan emas': 'Gold thickness',\n","                    'ketebalan TiO2': 'TiO2 thickness', \n","                    'confinement Loss': 'Confinement loss'}\n","\n","# rename the columns using the translation dictionary\n","df = df.rename(columns=translation_dict)"],"metadata":{"id":"W4v2PzDYC3XI","executionInfo":{"status":"ok","timestamp":1682124074513,"user_tz":-420,"elapsed":405,"user":{"displayName":"Andi Syamsul","userId":"15911607600703468816"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#kombinasi epoch dan nodes 10 50 100 150 200 250 \n","\n","import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from sklearn.preprocessing import StandardScaler\n","from keras.layers import Dense\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the data\n","data = df\n","\n","# Preprocess the data\n","X = data.iloc[:, :-1].values #data input\n","y = data.iloc[:, -1].values #data output\n","\n","# Split the data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","#define number of nodes \n","num_nodes_list = [10, 50, 100, 150, 200, 250]\n","\n","best_input_node = 0\n","best_mse = float('inf')\n","\n","for num_nodes in num_nodes_list :\n","    \n","    # Create the deep learning model\n","    model = Sequential()\n","    model.add(Dense(units= num_nodes, input_dim=5, activation='relu')) #input layer, how \n","    model.add(Dense(units= 10, activation='relu'))   #hidden layer 1\n","    model.add(Dense(units=1, activation='relu'))   #output layer\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    # Train the model\n","    history = model.fit(X_train_scaled, y_train, batch_size=10, epochs=10, verbose = 0)\n","\n","    # Evaluate the model\n","    score = model.evaluate(X_test_scaled, y_test)\n","    print('Test loss for {} nodes: {}'.format(num_nodes, score))\n","\n","    # Make predictions\n","    y_pred = model.predict(X_test_scaled)\n","\n","    # calculate the mean squared error\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(\"Mean Squared Error for {} nodes: {:.4f}\".format(num_nodes, mse))\n","    \n","    if mse < best_mse :\n","        best_mse = mse\n","        best_input_node = num_nodes\n","\n","    # mengekstrak eppoct dan mse dari history\n","    epoch = np.arange(1, len(history.history['loss']) + 1) \n","    mse = history.history['loss'] #Create a pandas dataframe to store the epoch and MSE \n","\n","    df_history = pd.DataFrame({'Epoch': epoch, 'MSE': mse})\n","\n","    #saving\n","    filename = 'training_history_{}nodes.csv'.format(num_nodes)\n","    df_history.to_csv(filename, index = False)\n","    \n","print(f'Best best_input_node : {best_input_node}, mse : {best_mse}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SnEI7XD8DJur","executionInfo":{"status":"ok","timestamp":1682124108725,"user_tz":-420,"elapsed":19666,"user":{"displayName":"Andi Syamsul","userId":"15911607600703468816"}},"outputId":"8d81e2bf-016b-4afe-8396-7a9cdb1edadb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["5/5 [==============================] - 0s 3ms/step - loss: 0.0713\n","Test loss for 10 nodes: 0.07131628692150116\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 10 nodes: 0.0713\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0312\n","Test loss for 50 nodes: 0.031165581196546555\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 50 nodes: 0.0312\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0365\n","Test loss for 100 nodes: 0.036488134413957596\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 100 nodes: 0.0365\n","5/5 [==============================] - 0s 3ms/step - loss: 0.7963\n","Test loss for 150 nodes: 0.7962594032287598\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 150 nodes: 0.7963\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0309\n","Test loss for 200 nodes: 0.030889946967363358\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 200 nodes: 0.0309\n","5/5 [==============================] - 0s 5ms/step - loss: 0.0378\n","Test loss for 250 nodes: 0.0378308929502964\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 250 nodes: 0.0378\n","Best best_input_node : 200, mse : 0.030889945666429212\n"]}]},{"cell_type":"code","source":["#kombinasi epoch dan nodes 10 50 100 150 200 250 \n","\n","import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from sklearn.preprocessing import StandardScaler\n","from keras.layers import Dense\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the data\n","data = df\n","\n","# Preprocess the data\n","X = data.iloc[:, :-1].values #data input\n","y = data.iloc[:, -1].values #data output\n","\n","# Split the data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","#define number of nodes \n","num_nodes_list = [10, 50, 100, 150, 200, 250]\n","\n","best_node = 0\n","best_mse = float('inf')\n","\n","for num_nodes in num_nodes_list :\n","    \n","    # Create the deep learning model\n","    model = Sequential()\n","    model.add(Dense(units= best_input_node, input_dim=5, activation='relu')) #input layer\n","    model.add(Dense(units=num_nodes, activation='relu'))   #hidden layer 1\n","    model.add(Dense(units=1, activation='relu'))   #output layer\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    # Train the model\n","    history = model.fit(X_train_scaled, y_train, batch_size=10, epochs=10, verbose = 0)\n","\n","    # Evaluate the model\n","    score = model.evaluate(X_test_scaled, y_test)\n","    print('Test loss for {} nodes: {}'.format(num_nodes, score))\n","\n","    # Make predictions\n","    y_pred = model.predict(X_test_scaled)\n","\n","    # calculate the mean squared error\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(\"Mean Squared Error for {} nodes: {:.4f}\".format(num_nodes, mse))\n","    \n","    if mse < best_mse :\n","        best_mse = mse\n","        best_node = num_nodes\n","\n","    # mengekstrak eppoct dan mse dari history\n","    epoch = np.arange(1, len(history.history['loss']) + 1) \n","    mse = history.history['loss'] #Create a pandas dataframe to store the epoch and MSE \n","\n","    df_history = pd.DataFrame({'Epoch': epoch, 'MSE': mse})\n","\n","    #saving\n","    filename = 'training_history_{}nodes.csv'.format(num_nodes)\n","    df_history.to_csv(filename, index = False)\n","    \n","print(f'Best best_node: {best_node}, mse : {best_mse}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83Y0Sx2EDVBp","executionInfo":{"status":"ok","timestamp":1682124137112,"user_tz":-420,"elapsed":16533,"user":{"displayName":"Andi Syamsul","userId":"15911607600703468816"}},"outputId":"c365cd66-a272-4a67-f248-09480d783bd9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["5/5 [==============================] - 0s 4ms/step - loss: 0.7963\n","Test loss for 10 nodes: 0.7962594032287598\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 10 nodes: 0.7963\n","5/5 [==============================] - 0s 4ms/step - loss: 0.0235\n","Test loss for 50 nodes: 0.02352362871170044\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 50 nodes: 0.0235\n","5/5 [==============================] - 0s 4ms/step - loss: 0.0191\n","Test loss for 100 nodes: 0.01905031129717827\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 100 nodes: 0.0191\n","5/5 [==============================] - 0s 4ms/step - loss: 0.0141\n","Test loss for 150 nodes: 0.01408196147531271\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 150 nodes: 0.0141\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0280\n","Test loss for 200 nodes: 0.027956459671258926\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 200 nodes: 0.0280\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0178\n","Test loss for 250 nodes: 0.017835600301623344\n","5/5 [==============================] - 0s 2ms/step\n","Mean Squared Error for 250 nodes: 0.0178\n","Best best_node: 150, mse : 0.014081962162835327\n"]}]},{"cell_type":"code","source":["#number of hidden layer dari 1 hingga 5 ( 1, 2, 3, 4, 5)\n","import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from sklearn.preprocessing import StandardScaler\n","from keras.layers import Dense\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the data\n","data = df\n","\n","# Preprocess the data\n","X = data.iloc[:, :-1].values #data input\n","y = data.iloc[:, -1].values #data output\n","\n","# Split the data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","#define number of nodes \n","num_layers_list = [1, 2, 3, 4, 5]\n","\n","best_mse = float('inf')\n","best_hidden_layer = 0\n","\n","for num_layers in num_layers_list :\n","    \n","    # Create the deep learning model\n","    model = Sequential()\n","    model.add(Dense(units= best_input_node , input_dim=5, activation='relu')) #input layer\n","    for i in range (num_layers):\n","        model.add(Dense(units=best_node, activation='relu'))   #hidden layer 1\n","    model.add(Dense(units=1, activation='relu'))   #output layer\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    # Train the model\n","    history = model.fit(X_train_scaled, y_train, batch_size=10, epochs=10, verbose = 0)\n","\n","    # Evaluate the model\n","    score = model.evaluate(X_test_scaled, y_test)\n","    print('Test loss for {} layers: {}'.format(num_layers, score))\n","\n","    # Make predictions\n","    y_pred = model.predict(X_test_scaled)\n","\n","    # calculate the mean squared error\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(\"Mean Squared Error for {} layers: {:.4f}\".format(num_layers, mse))\n","    \n","    # Check if the current epoch gives a lower loss\n","    if mse < best_mse:\n","        best_mse = mse\n","        best_hidden_layer = num_layers\n","\n","    # mengekstrak eppoct dan mse dari history\n","    epoch = np.arange(1, len(history.history['loss']) + 1) \n","    mse = history.history['loss'] #Create a pandas dataframe to store the epoch and MSE \n","\n","    # print(f\"this is me just checking mse: {mse}\")\n","    df_history = pd.DataFrame({'Epoch': epoch, 'MSE': mse})\n","    \n","    #saving\n","    filename = 'training_history_{}layers.csv'.format(num_layers)\n","\n","    df_history.to_csv(filename, index = False)\n","    \n","print(f'Best hidden_layer: {best_hidden_layer}, mse : {best_mse}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQRXncuuDYDp","executionInfo":{"status":"ok","timestamp":1682124158728,"user_tz":-420,"elapsed":17265,"user":{"displayName":"Andi Syamsul","userId":"15911607600703468816"}},"outputId":"c84a5287-2c00-4ed7-b018-561b1753aced"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["5/5 [==============================] - 0s 3ms/step - loss: 0.0234\n","Test loss for 1 layers: 0.02340204454958439\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 1 layers: 0.0234\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0120\n","Test loss for 2 layers: 0.012000370770692825\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 2 layers: 0.0120\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0218\n","Test loss for 3 layers: 0.02179660275578499\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 3 layers: 0.0218\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0214\n","Test loss for 4 layers: 0.021430691704154015\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 4 layers: 0.0214\n","5/5 [==============================] - 0s 3ms/step - loss: 0.0102\n","Test loss for 5 layers: 0.010183009319007397\n","5/5 [==============================] - 0s 3ms/step\n","Mean Squared Error for 5 layers: 0.0102\n","Best hidden_layer: 5, mse : 0.01018300939571858\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from sklearn.preprocessing import StandardScaler\n","from keras.layers import Dense\n","from sklearn.metrics import mean_squared_error\n","import openpyxl\n","\n","# Load the data\n","data = df\n","\n","# Preprocess the data\n","X = data.iloc[:, :-1].values\n","y = data.iloc[:, -1].values\n","\n","# Split the data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Create the deep learning model\n","model = Sequential()\n","model.add(Dense(units= best_input_node , input_dim=5, activation='relu')) #input layer\n","for i in range (best_hidden_layer):\n","    model.add(Dense(units=best_node, activation='relu'))   #hidden layer 1\n","model.add(Dense(units=1, activation='relu'))   #output layer\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Define epochs to train\n","epochs = [1000, 2000, 3000, 4000, 5000]\n","# epochs = [10, 20]\n","\n","# Train the model for each epoch and evaluate\n","best_mse = float('inf')\n","best_epoch = 0\n","for epoch in epochs:\n","    print(f'Training for {epoch} epochs')\n","    history = model.fit(X_train_scaled, y_train, batch_size = 10, epochs=epoch, verbose = 0)\n","    score = model.evaluate(X_test_scaled, y_test, verbose=0)\n","    print('Test loss:', score)\n","    \n","    # Make predictions\n","    y_pred = model.predict(X_test_scaled)\n","    \n","    # calculate the mean squared error\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(\"Mean Squared Error for {} epoch: {:.4f}\".format(epoch, mse))\n","    \n","    # Check if the current epoch gives a lower loss\n","    if mse < best_mse:\n","        best_mse = mse\n","        best_epoch = epoch\n","        \n","        # save the model\n","        model.save('best_model.h5')\n","    \n","    # mengekstrak epoch dan mse dari history\n","    epoch_i = np.arange(1, len(history.history['loss']) + 1) \n","    mse = history.history['loss'] #Create a pandas dataframe to store the epoch and MSE \n","    \n","    df_history = pd.DataFrame({'Epoch': epoch_i, 'MSE': mse})\n","    \n","    # saving the training history\n","    filename = 'training_history_{}epoch.csv'.format(epoch)\n","    df_history.to_csv(filename, index = False)\n","    \n","# load the best model\n","from keras.models import load_model\n","best_model = load_model('best_model.h5')\n","\n","# predict values for new data\n","new_data = data.iloc[:, :-1].values\n","y_new = data.iloc[:, -1].values\n","X_new = scaler.transform(new_data) # assuming new_data is a numpy array with the same number of features as X\n","y_new_pred = best_model.predict(X_new)\n","\n","# save the predicted and actual values in an Excel file\n","df_new = pd.DataFrame({'Actual': y_new, 'Predicted': y_new_pred.reshape(-1)})\n","filename = 'new_data_predictions.csv'\n","df_new.to_csv(filename, index=False)\n","\n","print(f'Best epoch: {best_epoch}, mse: {best_mse}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"v_35aCDbDauS","executionInfo":{"status":"error","timestamp":1682124229216,"user_tz":-420,"elapsed":21293,"user":{"displayName":"Andi Syamsul","userId":"15911607600703468816"}},"outputId":"5658f251-eb31-49db-bb73-22fcfcec3546"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training for 1000 epochs\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-7e827c83b24f>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training for {epoch} epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}